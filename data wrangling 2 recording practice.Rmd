---
title: "data-wrangling 2 practice"
author: "Seonyoung Park (sp3804)"
date: "11/1/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(httr)
```

## Scrap a table

I want the first table from [here]http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm]

read in the html

```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"
drug_use_html = read_html(url)

```

1. extract the table(s); from the website, I've extracted all the tables
2. I'm gonna focus on the first one
3. get html table
4. cut the first row (the â€œnoteâ€ at the bottom of the table appears in every column in the first row.)
5. change it to table

```{r}
table_mar = 
  drug_use_html %>%
  html_nodes(css = "table") %>%
  first() %>%
  html_table %>%
  slice(-1) %>%
  as_tibble()

table_mar
```

## Star Wars Movie Info

I want the data from [here](https://www.imdb.com/list/ls070150896/).

```{r}
url="https://www.imdb.com/list/ls070150896/"

swm_html = read_html(url)

```

Grab elements that I want- using SelectorGadget
1. Extract each vector
2. Make a table using all the vectors. 

```{r}
title_vec = 
  swm_html %>%
  html_nodes(css = ".lister-item-header a") %>%
  html_text()

gross_rev_vec = 
  swm_html %>%
  html_nodes(css = ".text-muted .ghost~ .text-muted+ span") %>%
  html_text()

runtime_vec = 
  swm_html %>%
  html_nodes(css = ".runtime") %>%
  html_text()

swm_df = 
  tibble(
    title = title_vec,
    gross_rev = gross_rev_vec,
    runtime = runtime_vec
  )
```

## Get some water data

1. This is coming from an API; if we can use csv, always use csv first. If not, use Jason, but more work to do. 

2. content("parsed") make a tibble. 

```{r}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>%
  content("parsed")

nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.json") %>% 
  content("text") %>%
  jsonlite::fromJSON() %>%
  as_tibble()

```


## BRFSS

Same process, different data

```{r}
brfss_2010 = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv") %>%
  content("parsed")

```

By default, the CDC API limits data to the first 1000 rows. Here Iâ€™ve increased that by changing an element of the API query. 
I looked around the website describing the API to find the name of the argument, and then used the appropriate syntax for GET.
To get the full data, I could increase this so that I get all the data at once or I could try iterating over chunks of a few thousand rows.

* The "$limit" parameter chooses how many records to return per page, and "$offset" tells the API on what record to start returning data.
* GET("http://google.com/", path = "search", query = list(q = "ham"))

```{r}
brfss_2010 = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv", query = list("$limit"=5000)) %>%
  content("parsed")

```


